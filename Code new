library(brms)
library(bridgesampling)
library(ggplot2)
library(reshape2)
library(dplyr)
library(gridExtra)
library(purrr)


# Clean data
df <- na.omit(airquality)
library(brms)
library(bridgesampling)

# Clean data: remove missing values
df <- na.omit(airquality)
# --------------- Priors-----------------------------
priors_linear <- c(
  set_prior("normal(1, 1)", class = "Intercept"),
  set_prior("normal(2, 1)", class = "b", coef = "x"),
  set_prior("exponential(1)", class = "sigma")
)

priors_quadratic <- c(
  set_prior("normal(1, 1)", class = "Intercept"),
  set_prior("normal(2, 1)", class = "b", coef = "x"),
  set_prior("normal(1.5, 1)", class = "b", coef = "x2"),
  set_prior("exponential(1)", class = "sigma")
)

fit_and_bf <- function(df) {
  fit_lin <- brm(
    y ~ x,
    data = df,
    prior = priors_linear,
    chains = 4, iter = 2000, warmup = 500,
    save_pars = save_pars(all = TRUE), refresh = 0
  )
  fit_quad <- brm(
    y ~ x + x2,
    data = df,
    prior = priors_quadratic,
    chains = 4, iter = 2000, warmup = 500,
    save_pars = save_pars(all = TRUE), refresh = 0
  )
  b_lin  <- bridge_sampler(fit_lin, silent = TRUE)
  b_quad <- bridge_sampler(fit_quad, silent = TRUE)
  bf_val <- bf(b_quad, b_lin)$bf
  return(bf_val)
}


# ---  repeat for different n ---
n_values <- c(5, 10, 20, 50, 100)
reps <- 5  

results <- map_dfr(n_values, function(n) {
  map_dfr(1:reps, function(r) {
    x <- rnorm(n)
    y <- 1 + 2*x + 1.5*x^2 + rnorm(n, sd = 1)
    df <- data.frame(x = x, x2 = x^2, y = y)
    bf_val <- fit_and_bf(df)
    tibble(n = n,
           rep = r,
           BF = bf_val,
           log10_BF = log10(bf_val),
           selects_quadratic = as.integer(bf_val > 1))
  })
})

# --- Summaries ---
summary_res <- results %>%
  group_by(n) %>%
  summarise(prob_select_quad = mean(selects_quadratic),
            mean_log10_BF = mean(log10_BF),
            sd_log10_BF = sd(log10_BF),
            .groups = "drop")

print(summary_res)

# --- Plot both side by side ---
par(mfrow = c(1, 2))  # 1 row, 2 plots

# Plot 1: Probability of selecting quadratic
plot(summary_res$n, summary_res$prob_select_quad, type = "b", pch = 19,
     ylim = c(0, 1),
     xlab = "Sample size (n)",
     ylab = "P(select quadratic model)",
     main = "Selection Probability")
abline(h = 0.5, lty = 2, col = "red")

# Plot 2: Mean log10 Bayes Factor
plot(summary_res$n, summary_res$mean_log10_BF, type = "b", pch = 19,
     xlab = "Sample size (n)",
     ylab = "Mean log10(BF)",
     main = "Log10 Bayes Factor")
arrows(summary_res$n,
       summary_res$mean_log10_BF - summary_res$sd_log10_BF,
       summary_res$n,
       summary_res$mean_log10_BF + summary_res$sd_log10_BF,
       angle = 90, code = 3, length = 0.05, col = "gray")
abline(h = 0, lty = 2, col = "red") 


# Example 2

# ----------------------Gamma models----------------

G1 <- brm(Ozone ~ Temp,
          data = df,
          family = Gamma(link = "log"),
          chains = 4, iter = 2000, seed = 123)

G2 <- brm(Ozone ~ Temp + Wind,
          data = df,
          family = Gamma(link = "log"),
          chains = 4, iter = 2000, seed = 123)


# ------------------Log-Normal models----------------

L1 <- brm(Ozone ~ Temp,
          data = df,
          family = lognormal(),
          chains = 4, iter = 2000, seed = 123)

L2 <- brm(Ozone ~ Temp + Wind,
          data = df,
          family = lognormal(),
          chains = 4, iter = 2000, seed = 123)



bridge_gamma_m1 <- bridge_sampler(G1, silent = TRUE)
bridge_gamma_m2 <- bridge_sampler(G2, silent = TRUE)


bridge_lognorm_m1 <- bridge_sampler(L1, silent = TRUE)
bridge_lognorm_m2 <- bridge_sampler(L2, silent = TRUE)



# vector of all marginal likelihoods
marginal_likes <- c(bridge_gamma_m1$logml, bridge_gamma_m2$logml,
                    bridge_lognorm_m1$logml, bridge_lognorm_m2$logml)

model_names <- c("G1", "G2",  "L1", "L2")

# reslts  data frame
results_df <- data.frame(
  Model = model_names,
  LogML = marginal_likes,
  Family = c(rep("Gamma", 2), rep("Lognormal", 2)),
  Complexity = rep(c("Temp only", "Temp + Wind"), 2)
)


# Function to compute log Bayes factors
compute_bf_matrix <- function(logml_vector, model_names) {
  n <- length(logml_vector)
  bf_matrix <- matrix(0, nrow = n, ncol = n)
  
  for(i in 1:n) {
    for(j in 1:n) {
      bf_matrix[i,j] <- logml_vector[i] - logml_vector[j]  # log BF_{i,j}
    }
  }
  
  rownames(bf_matrix) <- model_names
  colnames(bf_matrix) <- model_names
  return(bf_matrix)
}

# Compute BF matrix
bf_matrix <- compute_bf_matrix(marginal_likes, model_names)

# Convert to long format for ggplot
bf_long <- melt(bf_matrix, varnames = c("Model1", "Model2"), value.name = "LogBF")

# PLOT 1: HEATMAP ALL Models Comparison 

plot1_heatmap <- ggplot(bf_long, aes(x = Model2, y = Model1, fill = LogBF)) +
  geom_tile() +
  geom_text(aes(label = round(LogBF, 2)), color = "white", size = 3) +
  scale_fill_gradient2(low = "red", mid = "white", high = "blue", 
                       midpoint = 0, name = "log(BF)") +
  labs(title = "Bayes Factor Matrix (log scale)",
       subtitle = "Positive values favor row model over column model",
       x = "Model (denominator)", y = "Model (numerator)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(plot1_heatmap)


# PLOT 2: Model Comparison within Families

# Prepare data for within-family comparison
gamma_data <- results_df[results_df$Family == "Gamma", ]
lognorm_data <- results_df[results_df$Family == "Lognormal", ]

# Gamma family comparison
plot2a_gamma <- ggplot(gamma_data, aes(x = Model, y = LogML)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  geom_text(aes(label = round(LogML, 2)), vjust = -0.5) +
  labs(title = "Gamma Models Comparison",
       x = "Model", y = "Log Marginal Likelihood") +
  theme_minimal()

# Lognormal family comparison  
plot2b_lognorm <- ggplot(lognorm_data, aes(x = Model, y = LogML)) +
  geom_col(fill = "coral", alpha = 0.7) +
  geom_text(aes(label = round(LogML, 2)), vjust = -0.5) +
  labs(title = "Lognormal Models Comparison",
       x = "Model", y = "Log Marginal Likelihood") +
  theme_minimal()

# Combine both plots
plot2_combined <- grid.arrange(plot2a_gamma, plot2b_lognorm, ncol = 2)


# PLOT 3: Distribution Family Comparison (Best Models)

# Find best model in each family
best_gamma <- gamma_data[which.max(gamma_data$LogML), ]
best_lognorm <- lognorm_data[which.max(lognorm_data$LogML), ]

best_models <- rbind(best_gamma, best_lognorm)

plot3_families <- ggplot(best_models, aes(x = Family, y = LogML, fill = Family)) +
  geom_col(alpha = 0.7) +
  geom_text(aes(label = paste0(Model, "\n", round(LogML, 2))), vjust = -0.5) +
  scale_fill_manual(values = c("Gamma" = "steelblue", "Lognormal" = "coral")) +
  labs(title = "Best Model Comparison by Distribution Family",
       x = "Distribution Family", y = "Log Marginal Likelihood") +
  theme_minimal() +
  theme(legend.position = "none")

print(plot3_families)

# PLOT 4: Comprehensive Comparison

plot4_all <- ggplot(results_df, aes(x = Model, y = LogML, fill = Family)) +
  geom_col(alpha = 0.7, position = "dodge") +
  geom_text(aes(label = round(LogML, 2)), vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("Gamma" = "steelblue", "Lognormal" = "coral")) +
  labs(title = "All Models Comparison",
       subtitle = "Higher log marginal likelihood indicates better model",
       x = "Model", y = "Log Marginal Likelihood") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(plot4_all)

# PLOT 5: Bayes Factor Comparison (Relative to baseline)

# Use simplest model (G1) as baseline
baseline_logml <- results_df$LogML[results_df$Model == "G1"]

results_df$LogBF_vs_G1 <- results_df$LogML - baseline_logml

plot5_relative <- ggplot(results_df, aes(x = Model, y = LogBF_vs_G1, fill = Family)) +
  geom_col(alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_text(aes(label = round(LogBF_vs_G1, 2)), vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("Gamma" = "steelblue", "Lognormal" = "coral")) +
  labs(title = "Bayes Factors Relative to G1 (Baseline)",
       subtitle = "Positive values indicate better than G1",
       x = "Model", y = "log(BF) vs G1") +
  theme_minimal()

print(plot5_relative)


\begin{table}[H]
\centering
\begin{tabular}{c c c c c}
\hline
$n$ & Prob. Select Quad & Mean $\log_{10}$ BF & SD $\log_{10}$ BF & Interpretation (Kass and Raftery)\\
\hline
5   & 1 & 0.776 & 1.582 & Barely worth mentioning \\
10  & 1 & 2.525 & 1.915 & Positive evidence \\
20  & 1 & 3.028 & 1.126 & Positive evidence \\
50  & 1 & 9.450 & 0.782 &  Strong evidence \\
100 & 1 & 34.152 & 0.351 & Very strong evidence \\
\hline
\end{tabular}



Lets start by the following identity:
\begin{equation}
1 \;=\; 
\frac{\displaystyle \int_{\Theta} P(D \mid \theta, M)\,P(\theta \mid M)\,
    h(\theta \mid M)\,g(\theta \mid M)\,d\theta}
{\displaystyle \int_{\Theta} P(\theta \mid D, M)\,P(\theta \mid M)\,h(\theta \mid M)\,g(\theta \mid M)\,d\theta}.
\end{equation}

Here:
\begin{itemize}
    \item \(P(D \mid \theta, M)\) represents the likelihood of the data under the model M,
    \item \(P(\theta_i \mid M_i)\) is the prior distribution over the parameters.
    \item \(h(\theta \mid M)\) The bridge function
    \item \(g(\theta \mid M)\) The proposal distribution
\end{itemize}
Multiply both sides of equation (7) with \(P(D\mid M)\) to obtain:
\begin{equation}
  P(D\mid M)
= \frac{\displaystyle\int_{\Theta} P(D\mid\theta,M)\,P(\theta\mid M)\,h(\theta\mid M)\,g(\theta\mid M)\,d\theta}
{\displaystyle\int_{\Theta} \frac{P(D\mid\theta,M)\,P(\theta\mid M)}{P(D\mid M)}\,
h(\theta\mid M)\,g(\theta\mid M)\,d\theta}  
\end{equation}
where the quantity \(\frac{P(D\mid\theta,M)\,P(\theta\mid M)}{P(D\mid M)}\) is the posterior distribution according to Bayes theorem equation (1)

substitute equation (1) in equation (8) and reordering the terms :
\begin{equation}
P(D \mid M)
= \frac{\displaystyle \int_{\Theta} P(D \mid \theta, M)\,P(\theta \mid M)\,
  h(\theta \mid M)\,g(\theta \mid M)\, d\theta}
{\displaystyle \int_{\Theta} 
    h(\theta \mid M)\,g(\theta \mid M)\,P(\theta \mid D, M)\, d\theta}.
\end{equation}
Integrals against distributions represent the definition of the expectation 
\begin{equation}
  P(D\mid M)
= \frac{\mathbb{E}_{g(\theta\mid M)}[P(D\mid\theta,M)\,P(\theta\mid M)\,h(\theta\mid M)]}{\mathbb{E}_{P(\theta\mid D)}[h(\theta\mid M) g(\theta\mid M)]}
\end{equation}
\[
\underbrace{\{\hat{\theta}_i\}_{i=1}^{N_1} \sim g(\theta\mid M)}_{{\text{samples from proposal distribution}}} \quad \quad \underbrace{\{\tilde{\theta}_j\}_{j=1}^{N_2} \sim P(\theta\mid D)}_{{\text{samples from posterior distribution}}}\]

\begin{equation}
     P(D\mid M)
= \frac{\frac{1}{N_2} \sum_{i=1}^{N_2} {P(D\mid\tilde{\theta},M)\,P(\tilde{\theta}\mid M)\,h(\tilde{\theta}\mid M)}
} {\frac{1}{N_1} \sum_{i=1}^{N_1} {h(\hat{\theta}\mid M) g(\hat{\theta}\mid M)}} 
\end{equation}
